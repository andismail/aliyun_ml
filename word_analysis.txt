特征 编码：将非线性特征 通过GBDT编码成 线性特征
	大概意思是通过决策树和Ensemble算法做的一个编码策略，改策略可以有效的把非线性特征转换为线性特征
	一般配合GBDT二分类组件使用

窗口变量统计：给定时间窗口，计算相应用户在距离运行时间的时间窗内的运行次数和金额。
        	如时间窗口为‘1，7，30，90，180’，则计算用户相应天数内的行为次数和金额
大概的意思可能是说，如果用斐波那契数列来表示时间，会发现在这些时间上或附近总会出现一些奇怪的现象，如股市中的拐点
此处‘窗口变量’指的可能不单是‘时间窗口’，或者还有其它‘窗口’，窗口变量统计refer to把这些变量中出现的例如用户行为次数或金额拿出来，然后去预测接下来变量中的这些行为次数活金额

文本分析
1. SplitWord:基于AliWS(Alibaba Word Segmenter的简称)词法分析系统，对指定列对应的文章内容进行分词，分词后的各个词语间以空格作为分隔符，若用户指定了词性标注或语义标注相关参数，则会将分词结果、词性标注结果和语义标注结果一同输出，其中词性标注分隔符为”/“，语义标注分隔符为”|”。目前仅支持中文淘宝分词和互联网分词。

2. 三元组转kv:给定三元组（row,col,value）类型为XXD 或 XXL, X表示任意类型, D表示Double, L表示bigint，转成kv格式（row,[col_id:value]），其中row和value类型和原始输入数据一致，col_id类型是bigint，并给出col的索引表映射到col_id

3. 字符串相似度:用于检测两字符串中相似程度，提供三种算法

4. 字符串相似度-topN:无官方解释，猜测可能只是算法不同

5. 停用词过滤:是文本分析中一个预处理方法。它的功能是过滤分词结果中的噪声（例如：的、是、啊等）

6. ngram-count:ngram计数生成并操纵N-gram计数，并从它们估计N-gram语言模型。 该程序首先通过从文件读取计数或通过扫描文本输入来构建内部N-gram计数集。 之后，结果计数可以输出回到一个文件，或者用于构建一个ARRN格式的N-gram语言模型

7. 文本摘要:从原始文献中提取能准确地反映某一文献中心内容地简单连贯的短文,本算法基于TextRank，通过提取文档中已存在的句子形成摘要。
            常用流程: 原始语料 → 句子拆分 → 文本摘要

8. 关键词抽取:从文本里面把跟这篇文章意义最相关的一些词抽取出来.常用流程： 原始语料 → 分词 → 停用词过滤 → 关键词抽取
            本算法基于TextRank，它受到网页之间关系PageRank算法启发，利用局部词汇之间关系（共现窗口）构建网络，计算词的重要性，选取权重大的做为关键词

9. 句子拆分：无官方解释，猜测应该和‘文本摘要’、‘关键词抽取’功能类似，提取出文本中意义最相关的一些句子。

10. 语义向量距离:基于算法语义向量结果（如word2vec生成的词向量），计算给定的词（或者句子）的扩展词（或者句子），即计算其中某一向量距离最近的向量集合
	    其中一个用法是：基于word2vec生成的词向量结果，根据输入的词返回最为相似的词列表。

11. Doc2Vec:无官方解释，猜测和Word2vec类似，用来映射每个文档(Doc)到一个向量

12. 条件随机场:给定一组输入随机变量条件下,另一组输出随机变量的条件概率分布模型,其特点是假设输出随机变量构成马尔可夫随机场。
            条件随机场可以用于不同的预测问题，主要应用到标注问题中，其中线性链（linear chain）条件随机场是最典型的。

13. 文章相似度:文章相似度是在字符串相似度的基础上，基于词，计算两两文章或者句子之间的相似度，文章或者句子需要以空格分割的文本，计算方式和字符串相似度类似 
	    相似度类型同字符串相似度

14. PMI:PMI指数的英文全称为Purchasing Managers' Index，中文含义为采购经理指数，PMI指数50为荣枯分水线。当PMI大于50时，说明经济在发展，当PMI小于50时，说明经济在衰退。PMI是一套月度发布的、综合性的经济监测指标体系，分为制造业PMI、服务业PMI，也有一些国家建立了建筑业PMI。全球已有20多个国家建立了PMI体系，世界制造业和服务业PMI已经建立。PMI是通过对采购经理的月度调查汇总出来的指数，反映了经济的变化趋势
15. 条件随机场预测

16. 词频统计:在对文章进行分词的基础上,按行保序输出对应文章ID列(docId)对应文章的词，统计指定文章ID列(docId)对应文章内容(docContent)的词频。

17. tf: 在一份给定的文件里，词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率
idf: 逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量
有很多不同的数学公式可以用来计算tf-idf。这边的例子以上述的数学公式来计算。词频（tf）是一词语出现的次数除以该文件的总词语数。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。一个计算文件频率（DF）的方法是测定有多少份文件出现过“母牛”一词，然后除以文件集里包含的文件总数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是log（10,000,000 / 1,000）=4。最后的tf-idf的分数为0.03 * 4=0.12。
TF-IDF:是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。

18. PLDA:主题模型，返回文档对应的主题,隐含狄利克雷分布简称LDA(Latent Dirichlet allocation)，是一种主题模型，它可以将文档集中每篇文档的主题按照概率分布的形式给出。同时它是一种无监督学习算法，在训练时不需要手工标注的训练集，需要的仅仅是文档集以及指定主题的数量k即可。此外LDA的另一个优点则是，对于每一个主题均可找出一些词语来描述它。
LDA是一种典型的词袋模型，即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。

19. Word2vec为一群用来产生词嵌入的模型。这些模型为浅层和双层神经网络，用来训练以重新建构语言学之词文本，网络以词表现，并且需猜测相邻位置的输入词，在word2vec中bag-of-words 假设下，词的顺序是不重要的
训练完成之后，word2vec 模型可用来映射每个词到一个向量，可用来表示词对词之间的关系。该向量为神经网络之隐藏层
Word2vec 依赖skip-grams 或continuous-bag-of-words（CBOW）来建立神经词嵌入。Word2vec 为Tomas Mikolov 在 Google带领的研究团队创造。该算法渐渐被其他人所分析和解释


